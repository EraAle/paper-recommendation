import random
import openreview
from datetime import datetime
import time
import re
from .filtering import *

# v1ì€ 2022ê¹Œì§€ì˜ ìë£Œ ì°¾ê¸°
# accept ì§€ì› ë”°ë¡œ ì•ˆí•˜ì§€ë§Œ
# ë‚˜ì¤‘ì— í•„í„°ë§ ê±¸ ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ date, accept í•„í„°ë§ì— ì‚¬ìš©í•  ì •ë³´ë„ ê°™ì´ ë“¤ê³ ì˜¤ì
# --- OpenReview API v1 í¬ë¡¤ë§ í•¨ìˆ˜ (ì£¼ë¡œ ~2022ë…„ ë°ì´í„°) ---
# def crawling_openreview_v1(search_query: str, limit: int, date: list[int] = None, accept: bool = True) -> list[dict[str, any]]:
#     """
#     [êµ¬ ë²„ì „ API] OpenReviewì—ì„œ ~2022ë…„ê¹Œì§€ì˜ ë…¼ë¬¸ì„ í˜ì´ì§•í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
#     - limitì´ 100ì„ ì´ˆê³¼í•˜ë©´ 100ê°œì”© ë‚˜ëˆ„ì–´ ìš”ì²­í•˜ê³  3ì´ˆì”© ëŒ€ê¸°í•©ë‹ˆë‹¤.
#     """
#     print("--- OpenReview v1 APIë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ (~2022ë…„ ë°ì´í„°) ---")
#     PAGE_SIZE = 100
#     documents = []
#     offset = 0
#     client = openreview.Client(baseurl='https://api.openreview.net')
#
#     # 2022ë…„ 12ì›” 31ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ ì´ì „ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
#     end_date_v1 = datetime(2023, 12, 31, 23, 59, 59)
#     maxtcdate_v1 = int(time.mktime(end_date_v1.timetuple()) * 1000)
#
#     try:
#         while len(documents) < limit:
#             # ì´ë²ˆ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ ê°œìˆ˜ ê³„ì‚°
#             remaining_limit = limit - len(documents)
#             current_page_limit = min(PAGE_SIZE, remaining_limit)
#
#             print(f"v1 API: {offset}ë²ˆì§¸ë¶€í„° {current_page_limit}ê°œì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤...")
#
#             if date is None:
#                 # offsetê³¼ limitì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§• ìˆ˜í–‰
#                 page_results = client.get_notes(
#                     query=search_query,
#                     limit=current_page_limit,
#                     offset=offset,
#                     maxtcdate=maxtcdate_v1
#                 )
#             else:
#                 end_date_v1 = datetime(date[1], 12, 31, 23, 59, 59)
#                 start_date_v1 = datetime(date[0], 1, 1, 00, 00, 00)
#
#                 maxtcdate_v1 = int(time.mktime(end_date_v1.timetuple()) * 1000)
#                 mintcdate_v1 = int(time.mktime(start_date_v1.timetuple()) * 1000)
#
#                 page_results = client.get_notes(
#                     query=search_query,
#                     limit=current_page_limit,
#                     offset=offset,
#                     mintcdate=mintcdate_v1,
#                     maxtcdate=maxtcdate_v1
#                 )
#
#             # ë” ì´ìƒ ê°€ì ¸ì˜¬ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
#             if not page_results:
#                 print("ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
#                 break
#
#             for note in page_results:
#                 decision = note.content.get('decision', {}).get('value', '')
#                 venue = note.content.get('venue', {}).get('value', '')
#                 documents.append({
#                     'title': note.content.get('title', 'N/A'),
#                     'url': f"https://openreview.net/forum?id={note.id}",
#                     'abstract': note.content.get('abstract', 'N/A'),
#                     'cdate': note.cdate,
#                     'decision_info': decision or venue
#                 })
#
#             offset += len(page_results)
#
#             if accept == True:
#                 documents = v1_accept_filter(documents)
#
#             # limitì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ê³ , ê°€ì ¸ì˜¨ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë‹¤ìŒ ìš”ì²­ ì „ì— ëŒ€ê¸°
#             if len(documents) < limit and page_results:
#                 print(f"... ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ. 3ì´ˆê°„ ëŒ€ê¸°í•©ë‹ˆë‹¤ ...")
#                 time.sleep(3)
#
#         print(f"v1 APIì—ì„œ ìµœì¢… {len(documents)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
#
#     except Exception as e:
#         print(f"v1 API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#
#     return documents
def crawling_openreview_v1(search_query: any, limit: int, date: list[int] = None, accept: bool = True) -> list[
    dict[str, any]]:
    print("--- OpenReview v1 APIë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ (~2022ë…„ ë°ì´í„°) ---")
    PAGE_SIZE = 100
    documents = []
    offset = 0
    client = openreview.Client(baseurl='https://api.openreview.net')

    # ğŸ› ï¸ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
    params = {'limit': PAGE_SIZE, 'offset': offset}
    if isinstance(search_query, str):
        params['term'] = search_query  # ğŸ› ï¸ query -> term
    elif isinstance(search_query, dict):
        params['content'] = search_query  # ğŸ› ï¸ plan_openreview_v1_queriesì˜ ê²°ê³¼(ë”•ì…”ë„ˆë¦¬) ì²˜ë¦¬

    # ë‚ ì§œ ì„¤ì • (v1ì€ mintcdate, maxtcdateê°€ ì•„ì§ ìœ íš¨)
    if date:
        params['mintcdate'] = int(datetime(date[0], 1, 1).timestamp() * 1000)
        params['maxtcdate'] = int(datetime(date[1], 12, 31, 23, 59, 59).timestamp() * 1000)
    else:
        # ê¸°ë³¸ê°’: 2022ë…„ 12ì›” 31ì¼ ì´ì „
        params['maxtcdate'] = int(datetime(2022, 12, 31, 23, 59, 59).timestamp() * 1000)

    try:
        while len(documents) < limit:
            params['limit'] = min(PAGE_SIZE, limit - len(documents))
            params['offset'] = offset

            print(f"v1 API: {offset}ë²ˆì§¸ë¶€í„° {params['limit']}ê°œì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤...")
            page_results = client.get_notes(**params)

            if not page_results:
                print("ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

            # ... (ê¸°ì¡´ ë°ì´í„° ì¶”ê°€ ë° í•„í„°ë§ ë¡œì§) ...
            for note in page_results:
                decision = note.content.get('decision', {}).get('value', '')
                venue = note.content.get('venue', {}).get('value', '')
                documents.append({
                    'title': note.content.get('title', 'N/A'),
                    'url': f"https://openreview.net/forum?id={note.id}",
                    'abstract': note.content.get('abstract', 'N/A'),
                    'cdate': note.cdate,
                    'decision_info': decision or venue
                })

            offset += len(page_results)

            if accept:
                documents = v1_accept_filter(documents)  # v1_accept_filter í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •

            if len(documents) < limit and page_results:
                print(f"... ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ. 3ì´ˆê°„ ëŒ€ê¸°í•©ë‹ˆë‹¤ ...")
                time.sleep(3)

        print(f"v1 APIì—ì„œ ìµœì¢… {len(documents)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"v1 API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return documents

# def crawling_openreview_v2(
#         search_query: str,
#         limit: int,
#         date: list[int] = None,
#         accept: bool = True
# ) -> list[dict[str, any]]:
#     """
#     [ì‹  ë²„ì „ API] OpenReviewì—ì„œ íŠ¹ì • ê¸°ê°„ì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  accept ì—¬ë¶€ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.
#     - get_all_notes ëŒ€ì‹  search_notesë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#     - sort_op='relevance'ëŠ” search_notesì˜ ê¸°ë³¸ ë™ì‘ì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
#
#     Args:
#         search_query: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¬¸ìì—´ (ì˜ˆ: 'title:"large language model"')
#         limit: ê°€ì ¸ì˜¬ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
#         sort_op: ì •ë ¬ ì˜µì…˜ ('relevance'ê°€ ê¸°ë³¸ê°’).
#         date: ê²€ìƒ‰í•  ì—°ë„ ë²”ìœ„ [ì‹œì‘, ì¢…ë£Œ].
#         accept: Trueì´ë©´ Acceptëœ ë…¼ë¬¸ë§Œ í•„í„°ë§.
#
#     Returns:
#         ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸
#     """
#     print(f"--- OpenReview v2 APIë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
#     documents = []
#     client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')
#
#     # ë‚ ì§œ ë²”ìœ„ë¥¼ ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„(ms)ë¡œ ë³€í™˜
#     start_ts = int(datetime(2023, 1, 1).timestamp() * 1000)
#
#
#     try:
#         if date is None:
#             # get_all_notes ëŒ€ì‹  search_notesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#             search_iterator = client.search_notes(
#                 term=search_query,
#                 mintcdate=start_ts
#             )
#         else:
#             # ë‚ ì§œ ë²”ìœ„ë¥¼ ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„(ms)ë¡œ ë³€í™˜
#             start_ts = int(datetime(date[0], 1, 1).timestamp() * 1000)
#             end_ts = int(datetime(date[1], 12, 31, 23, 59, 59).timestamp() * 1000)
#
#             # get_all_notes ëŒ€ì‹  search_notesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#             search_iterator = client.search_notes(
#                 term=search_query,
#                 mintcdate=start_ts,
#                 maxtcdate=end_ts
#             )
#
#         for note in search_iterator:
#             # limitì— ë„ë‹¬í•˜ë©´ ê²€ìƒ‰ ì¤‘ë‹¨
#             if len(documents) >= limit:
#                 print(f"Limit({limit})ì— ë„ë‹¬í•˜ì—¬ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
#                 break
#
#             # accept=Trueì¼ ê²½ìš°, í•„í„°ë§ ë¡œì§ ìˆ˜í–‰
#             if accept:
#                 decision = note.content.get('decision', {}).get('value', '')
#                 venue = note.content.get('venue', {}).get('value', '')
#
#                 # decision ë˜ëŠ” venue í•„ë“œì— 'accept'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
#                 if re.search('accept', decision, re.IGNORECASE) or re.search('accept', venue, re.IGNORECASE):
#                     documents.append({
#                         'title': note.content.get('title', {}).get('value', 'N/A'),
#                         'url': f"https://openreview.net/forum?id={note.id}",
#                         'abstract': note.content.get('abstract', {}).get('value', 'N/A'),
#                         'cdate': note.cdate,
#                         'decision_info': decision or venue
#                     })
#             else:  # accept=Falseì´ë©´ í•„í„°ë§ ì—†ì´ ëª¨ë‘ ì¶”ê°€
#                 documents.append({
#                     'title': note.content.get('title', {}).get('value', 'N/A'),
#                     'url': f"https://openreview.net/forum?id={note.id}",
#                     'abstract': note.content.get('abstract', {}).get('value', 'N/A'),
#                     'cdate': note.cdate,
#                     'decision_info': ""  # í•„í„°ë§ ì•ˆí–ˆìœ¼ë¯€ë¡œ ë¹„ì›Œë‘ 
#                 })
#
#         print(f"v2 APIì—ì„œ ìµœì¢… {len(documents)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
#
#     except Exception as e:
#         print(f"v2 API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#
#     return documents

def crawling_openreview_v2(
        search_query: str,
        limit: int,
        date: list[int] = None,
        accept: bool = True
) -> list[dict[str, any]]:
    print(f"--- OpenReview v2 APIë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    documents = []
    client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')

    try:
        # ğŸ› ï¸ 'query' -> 'term' ë³€ê²½ ë° ë‚ ì§œ ê²€ìƒ‰ ë°©ì‹ ìˆ˜ì •
        params = {'term': search_query}

        if date is None:
            # 2023ë…„ 1ì›” 1ì¼ ì´í›„ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
            start_ts = int(datetime(2023, 1, 1).timestamp() * 1000)
            params['cdate'] = {'gte': start_ts}  # ğŸ› ï¸ mintcdate ëŒ€ì‹  cdate ì‚¬ìš©
        else:
            start_ts = int(datetime(date[0], 1, 1).timestamp() * 1000)
            end_ts = int(datetime(date[1], 12, 31, 23, 59, 59).timestamp() * 1000)
            params['cdate'] = {'gte': start_ts, 'lte': end_ts}  # ğŸ› ï¸ mintcdate, maxtcdate ëŒ€ì‹  cdate ì‚¬ìš©

        search_iterator = client.search_notes(**params)

        # ì´í•˜ ë¡œì§ì€ ë™ì¼
        for note in search_iterator:
            if len(documents) >= limit:
                break

            # ... (ê¸°ì¡´ accept í•„í„°ë§ ë° ë°ì´í„° ì¶”ê°€ ë¡œì§) ...
            if accept:
                decision = note.content.get('decision', {}).get('value', '')
                venue = note.content.get('venue', {}).get('value', '')
                if re.search('accept', decision, re.IGNORECASE) or re.search('accept', venue, re.IGNORECASE):
                    documents.append({
                        'title': note.content.get('title', {}).get('value', 'N/A'),
                        'url': f"https://openreview.net/forum?id={note.id}",
                        'abstract': note.content.get('abstract', {}).get('value', 'N/A'),
                        'cdate': note.cdate,
                        'decision_info': decision or venue
                    })
            else:
                documents.append({
                    'title': note.content.get('title', {}).get('value', 'N/A'),
                    'url': f"https://openreview.net/forum?id={note.id}",
                    'abstract': note.content.get('abstract', {}).get('value', 'N/A'),
                    'cdate': note.cdate,
                    'decision_info': ""
                })

        print(f"v2 APIì—ì„œ ìµœì¢… {len(documents)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"v2 API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return documents


def crawling_openreview_mix(search_query: str, search_query_v1, limit: int = 50, date: list[int] = [2021, 2025], accept = True):
    documents_v1 = []
    documents_v2 = []

    client_v1 = openreview.Client(baseurl='https://api.openreview.net')
    client_v2 = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')

    # ì–¼ë§ˆì”© ë‚˜ëˆ ì„œ ê°€ì ¸ì˜¬ì§€ ê²°ì •
    year_v1 = 2023 - date[0]
    year_v2 = date[1] - 2022

    total_year = year_v1 + year_v2

    # ë³€í™˜í•œ limit
    limit_v1 = round((year_v1 / total_year) * limit)
    limit_v2 = round((year_v2 / total_year) * limit)

    # í˜¹ì‹œ ëª¨ë¥¼ ë°˜ì˜¬ë¦¼ ì˜¤ë¥˜ë¥¼ ìœ„í•´, ì´í•©ì´ total_limitì„ ë„˜ì§€ ì•Šë„ë¡ ë³´ì •
    if limit_v1 + limit_v2 > limit:
        limit_v1 = limit - limit_v2
    if limit_v1 <= 0: limit_v1 = 1  # ìµœì†Œ 1ê°œëŠ” ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •

    date_v1 = [date[0], 2022]
    date_v2 = [2023, date[1]]

    documents_v1 = crawling_openreview_v1(search_query_v1, limit_v1, date_v1, accept=True)
    documents_v2 = crawling_openreview_v2(search_query, limit_v2, date_v2, accept=True)

    return documents_v1 + documents_v2