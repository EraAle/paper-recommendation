{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e61b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"당신은 정확한 연구 요약가입니다. 출력은 GitHub‑Flavored Markdown으로 작성합니다. 사적 추론()은 절대 출력하지 마세요. 최종 답변은 <final> ... </final> 로 감싸서 출력하라오직 최종결과만 한국어로 작성합니다. 숫자/성능치는 입력 JSON의 evidence.numbers에 있는 값만 사용하고, 없으면 '원문에 수치 미기재'라고 명시합니다. 링크는 입력 url만 사용하고, '~입니다' 톤으로 간결하게 작성합니다.\n",
    "user\n",
    "아래 JSON의 user_instruction과 papers를 바탕으로,\n",
    "각 논문을 '리서치 카드' 형식으로 요약하고, 맨 마지막에 Top‑k 비교표 1개를 추가하세요.\n",
    "\n",
    "요구사항:\n",
    "- 카드 필수 항목:\n",
    "  1) 제목(연도, venue, 링크) 2) TL;DR 3) 왜 관련? (2–3개) 4) 핵심 기여(3–4불릿)\n",
    "  5) 한계/주의(1–2불릿) 6) 난이도(초/중/고), 예상 읽기 시간(분) 7) 해시태그 3–5개\n",
    "- (선택) scores.hybrid/dense/ce 중 2개까지 '(CE=0.91, Dense=0.83)' 형태로 표기합니다.\n",
    "- 숫자/성능치는 evidence.numbers만 사용하고 없으면 '원문에 수치 미기재'라고 씁니다.\n",
    "- 문장은 간결하게, 불릿 위주. TL;DR 30–50자, 카드당 불릿 6–9개.\n",
    "- 마지막 비교표(테이블) 열:\n",
    "  | 순위 | 제목 | 방법(한줄) | 데이터/스케일 | 핵심성능(있으면) | 관련성(0–5) | 난이도 | 링크 |\n",
    "\n",
    "입력 JSON:\n",
    "```\n",
    "json{\n",
    "  \"user_instruction\": \"RAG\",\n",
    "  \"papers\": []\n",
    "}\n",
    "papers 데이터:\n",
    "\n",
    "{\n",
    "  \"papers\": [\n",
    "    {\n",
    "      \"title\": \"A Simple Baseline for Multimodal RAG\",\n",
    "      \"url\": \"https://example.org/paper1\",\n",
    "      \"abstract\": \"We present a baseline that fuses image and text evidence for retrieval-augmented generation (RAG) on QA benchmarks.\",\n",
    "      \"venue\": \"\",\n",
    "      \"year\": \"\",\n",
    "      \"authors\": [],\n",
    "      \"keywords\": [],\n",
    "      \"scores\": {},\n",
    "      \"evidence\": {}\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Audio-RAG: Integrating Speech Evidence into Generative QA\",\n",
    "      \"url\": \"https://example.org/paper6\",\n",
    "      \"abstract\": \"We incorporate ASR transcripts and acoustic cues to improve RAG for spoken question answering.\",\n",
    "      \"venue\": \"\",\n",
    "      \"year\": \"\",\n",
    "      \"authors\": [],\n",
    "      \"keywords\": [],\n",
    "      \"scores\": {},\n",
    "      \"evidence\": {}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "assistant\n",
    "\n",
    "\n",
    "<final>\n",
    "# 리서치 카드 요약\n",
    "\n",
    "## 1. A Simple Baseline for Multimodal RAG\n",
    "**제목**: A Simple Baseline for Multimodal RAG  \n",
    "**연도**: N/A  \n",
    "**발행처**: N/A  \n",
    "**링크**: [https://example.org/paper1](https://example.org/paper1)  \n",
    "**TL;DR**: 다중 모드 데이터 통합을 통해 RAG 기반 질문 답변 성능 향상  \n",
    "**왜 관련?**: RAG에서 이미지와 텍스트 데이터 통합이 성능 향상에 기여함  \n",
    "**핵심 기여**: 다중 모드 데이터 통합, QA 데모 및 평가  \n",
    "**한계/주의**: 특정 모드의 상세 정보 부족  \n",
    "**난이도**: 중 (10~15분)  \n",
    "**해시태그**: #RAG #Multimodal #QA  \n",
    "\n",
    "## 2. Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
    "**제목**: Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
    "**연도**: N/A  \n",
    "**발행처**: N/A  \n",
    "**링크**: [https://example.org/paper6](https://example.org/paper6)  \n",
    "**TL;DR**: 음성 데이터 통합을 통해 음성 질문 답변 성능 개선  \n",
    "**왜 관련?**: 음성 질문이 주요 문제 유형, 음성 처리 기술 필요  \n",
    "**핵심 기여**: 음성 인식(transcripts) 및 악성 신호 처리 통합  \n",
    "**한계/주의**: ASR 정확도 의존성  \n",
    "**난이도**: 중 (15~20분)  \n",
    "**해시태그**: #AudioRAG #SpokenQA #SpeechProcessing  \n",
    "\n",
    "## Top-K 비교표\n",
    "| 순위 | 제목 | 방법(한줄) | 데이터/스케일 | 핵심성능(있으면) | 관련성(0–5) | 난이도 | 링크 |\n",
    "|------|------|-------------|----------------|------------------|------------|-------|------|\n",
    "| 1    | A Simple Baseline for Multimodal RAG | 다중 모드 데이터 통합 | QA 데모 | RAG 성능 향상 | 4/5 | 중 | [paper1](https://example.org/paper1) |\n",
    "| 2    | Audio-RAG: Integrating Speech Evidence into Generative QA | 음성 데이터 통합 | 음성 QA | 음성 질문 처리 | 4/5 | 중 | [paper6](https://example.org/paper6) |\n",
    "</final>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "305c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "FINAL_RE = re.compile(r\"<\\s*final\\s*>(.*?)<\\s*/\\s*final\\s*>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "def extract_final(text: str) -> str:\n",
    "    \"\"\"\n",
    "    <final> ... </final> 블록 안의 내용을 반환합니다.\n",
    "    해당 블록이 없으면 원문 전체를 반환합니다.\n",
    "    \"\"\"\n",
    "    final_contents = re.findall(r\"<final>(.*?)</final>\", text, re.DOTALL)\n",
    "    if final_contents:\n",
    "        return final_contents[1].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d31cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 리서치 카드 요약\n",
      "\n",
      "## 1. A Simple Baseline for Multimodal RAG\n",
      "**제목**: A Simple Baseline for Multimodal RAG  \n",
      "**연도**: N/A  \n",
      "**발행처**: N/A  \n",
      "**링크**: [https://example.org/paper1](https://example.org/paper1)  \n",
      "**TL;DR**: 다중 모드 데이터 통합을 통해 RAG 기반 질문 답변 성능 향상  \n",
      "**왜 관련?**: RAG에서 이미지와 텍스트 데이터 통합이 성능 향상에 기여함  \n",
      "**핵심 기여**: 다중 모드 데이터 통합, QA 데모 및 평가  \n",
      "**한계/주의**: 특정 모드의 상세 정보 부족  \n",
      "**난이도**: 중 (10~15분)  \n",
      "**해시태그**: #RAG #Multimodal #QA  \n",
      "\n",
      "## 2. Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
      "**제목**: Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
      "**연도**: N/A  \n",
      "**발행처**: N/A  \n",
      "**링크**: [https://example.org/paper6](https://example.org/paper6)  \n",
      "**TL;DR**: 음성 데이터 통합을 통해 음성 질문 답변 성능 개선  \n",
      "**왜 관련?**: 음성 질문이 주요 문제 유형, 음성 처리 기술 필요  \n",
      "**핵심 기여**: 음성 인식(transcripts) 및 악성 신호 처리 통합  \n",
      "**한계/주의**: ASR 정확도 의존성  \n",
      "**난이도**: 중 (15~20분)  \n",
      "**해시태그**: #AudioRAG #SpokenQA #SpeechProcessing  \n",
      "\n",
      "## Top-K 비교표\n",
      "| 순위 | 제목 | 방법(한줄) | 데이터/스케일 | 핵심성능(있으면) | 관련성(0–5) | 난이도 | 링크 |\n",
      "|------|------|-------------|----------------|------------------|------------|-------|------|\n",
      "| 1    | A Simple Baseline for Multimodal RAG | 다중 모드 데이터 통합 | QA 데모 | RAG 성능 향상 | 4/5 | 중 | [paper1](https://example.org/paper1) |\n",
      "| 2    | Audio-RAG: Integrating Speech Evidence into Generative QA | 음성 데이터 통합 | 음성 QA | 음성 질문 처리 | 4/5 | 중 | [paper6](https://example.org/paper6) |\n"
     ]
    }
   ],
   "source": [
    "print(extract_final(text))  # Call the function to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e70b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "# 리서치 카드 요약\n",
      "\n",
      "## 1. A Simple Baseline for Multimodal RAG\n",
      "**제목**: A Simple Baseline for Multimodal RAG  \n",
      "**연도**: N/A  \n",
      "**발행처**: N/A  \n",
      "**링크**: [https://example.org/paper1](https://example.org/paper1)  \n",
      "**TL;DR**: 다중 모드 데이터 통합을 통해 RAG 기반 질문 답변 성능 향상  \n",
      "**왜 관련?**: RAG에서 이미지와 텍스트 데이터 통합이 성능 향상에 기여함  \n",
      "**핵심 기여**: 다중 모드 데이터 통합, QA 데모 및 평가  \n",
      "**한계/주의**: 특정 모드의 상세 정보 부족  \n",
      "**난이도**: 중 (10~15분)  \n",
      "**해시태그**: #RAG #Multimodal #QA  \n",
      "\n",
      "## 2. Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
      "**제목**: Audio-RAG: Integrating Speech Evidence into Generative QA  \n",
      "**연도**: N/A  \n",
      "**발행처**: N/A  \n",
      "**링크**: [https://example.org/paper6](https://example.org/paper6)  \n",
      "**TL;DR**: 음성 데이터 통합을 통해 음성 질문 답변 성능 개선  \n",
      "**왜 관련?**: 음성 질문이 주요 문제 유형, 음성 처리 기술 필요  \n",
      "**핵심 기여**: 음성 인식(transcripts) 및 악성 신호 처리 통합  \n",
      "**한계/주의**: ASR 정확도 의존성  \n",
      "**난이도**: 중 (15~20분)  \n",
      "**해시태그**: #AudioRAG #SpokenQA #SpeechProcessing  \n",
      "\n",
      "## Top-K 비교표\n",
      "| 순위 | 제목 | 방법(한줄) | 데이터/스케일 | 핵심성능(있으면) | 관련성(0–5) | 난이도 | 링크 |\n",
      "|------|------|-------------|----------------|------------------|------------|-------|------|\n",
      "| 1    | A Simple Baseline for Multimodal RAG | 다중 모드 데이터 통합 | QA 데모 | RAG 성능 향상 | 4/5 | 중 | [paper1](https://example.org/paper1) |\n",
      "| 2    | Audio-RAG: Integrating Speech Evidence into Generative QA | 음성 데이터 통합 | 음성 QA | 음성 질문 처리 | 4/5 | 중 | [paper6](https://example.org/paper6) |\n"
     ]
    }
   ],
   "source": [
    "final_contents = re.findall(r\"<final>(.*?)</final>\", text, re.DOTALL)\n",
    "\n",
    "for content in final_contents:\n",
    "    print(content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d25c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "text = \"<final>ABC</final>\"\n",
    "print(extract_final(text))       # 출력: ABC\n",
    "print(len(extract_final(text)))  # 출력: 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
