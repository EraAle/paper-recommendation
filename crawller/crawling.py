import arxiv
import requests
import re

# ê²€ìƒ‰ í‚¤ì›Œë“œ ë° ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì„¤ì •
# ì´ê±° ì“¸ ë•Œ ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ í‚¤ì›Œë“œëŠ” ""ë¡œë§Œ ì‘ì„±í•´ë‹¬ë¼ í•˜ê¸° ''ì€ ì•ˆë¨
# sort ì˜µì…˜ì€ relevance, lastupdate, submitted ì„¸ ê°€ì§€

def make_query(keyword_list, operator="AND", field = "title"):
    # 1. ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  í‚¤ì›Œë“œë¥¼ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤. (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜)
    #    'def gh'ì™€ ê°™ì€ êµ¬ë¬¸ ê²€ìƒ‰ì„ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
    quoted_keywords = [f'"{k}"' for k in keyword_list]

    if isinstance(operator, str):
        # 2. ì—°ì‚°ì ì–‘ì˜†ì— ê³µë°±ì„ ë„£ì–´ ' AND ' ë˜ëŠ” ' OR ' í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
        separator = f" {operator} "

        # 3. ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ í‚¤ì›Œë“œë“¤ì„ ì—°ì‚°ìë¡œ ì—°ê²°í•©ë‹ˆë‹¤. (join ë©”ì„œë“œ)
        search_query = separator.join(quoted_keywords)

    elif isinstance(operator, list):
        quoted_keywords = [f'"{k}"' for k in keyword_list]

        search_query = quoted_keywords[0]

        for op, keyword in zip(operator, quoted_keywords[1:]):
            search_query += f" {op} {keyword}"

    else:
        raise TypeError("operator TypeError")

    query = add_field(search_query, field)

    return query

def add_field(query, field):
    """
        ì¿¼ë¦¬ ë¬¸ìì—´ê³¼ í•„ë“œ(ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)ë¥¼ ë°›ì•„ ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        :param query: '"a" AND "b"' í˜•íƒœì˜ ê²€ìƒ‰ ì¿¼ë¦¬
        :param field: 'title', ['title', 'abstract'] ë“± í•„ë“œ ì •ë³´
        :return: ì ‘ë‘ì‚¬ê°€ ì¶”ê°€ëœ ìƒˆë¡œìš´ ì¿¼ë¦¬ ë¬¸ìì—´
        """
    # í•„ë“œ ì´ë¦„ê³¼ ì‹¤ì œ ì ‘ë‘ì‚¬ ë§¤í•‘
    PREFIX_MAP = {
        'title': 'ti',
        'abstract': 'abs',
        'all': 'all'
    }

    # 1. ì¿¼ë¦¬ë¥¼ í‚¤ì›Œë“œì™€ ì—°ì‚°ìë¡œ ë¶„ë¦¬
    parts = re.split(r' (AND|OR|ANDNOT) ', query)
    keywords = parts[::2]
    num_keywords = len(keywords)

    field_prefixes = []

    # 2. fields íŒŒë¼ë¯¸í„° íƒ€ì…ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
    if isinstance(field, str):
        # ë¬¸ìì—´ì¸ ê²½ìš°, ëª¨ë“  í‚¤ì›Œë“œì— ë™ì¼í•œ ì ‘ë‘ì‚¬ ì ìš©
        if field not in PREFIX_MAP:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œ ì´ë¦„ì…ë‹ˆë‹¤: {field}")
        prefix = PREFIX_MAP[field]
        field_prefixes = [prefix] * num_keywords

    elif isinstance(field, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°, ê° í‚¤ì›Œë“œì— ìˆœì„œëŒ€ë¡œ ì ‘ë‘ì‚¬ ì ìš©
        if num_keywords != len(field):
            raise ValueError(
                f"í‚¤ì›Œë“œ ê°œìˆ˜({num_keywords})ì™€ í•„ë“œ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´({len(field)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )
        # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•„ë“œ ì´ë¦„ì„ ì‹¤ì œ ì ‘ë‘ì‚¬ë¡œ ë³€í™˜
        field_prefixes = [PREFIX_MAP.get(f) for f in field]
        if None in field_prefixes:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œ ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {field}")

    else:
        raise TypeError("fields íŒŒë¼ë¯¸í„°ëŠ” ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # 3. ê° í‚¤ì›Œë“œ ì•ì— ë³€í™˜ëœ ì ‘ë‘ì‚¬ ì¶”ê°€
    for i, prefix in enumerate(field_prefixes):
        keyword_index = i * 2
        parts[keyword_index] = f"{prefix}:{parts[keyword_index]}"

    # 4. ëª¨ë“  ë¶€ë¶„ì„ ë‹¤ì‹œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    return " ".join(parts)

def crawling_basic(search_query, num=50, sort_op="relevance"):
    """
    Args:
        keyword_list (list): ê²€ìƒ‰í•  í‚¤ì›Œë“œê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ (e.g., ["abc", "def gh"])
        num (int): ê°€ì ¸ì˜¬ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
        operator (str): í‚¤ì›Œë“œë¥¼ ì—°ê²°í•  ë…¼ë¦¬ ì—°ì‚°ì ("AND", "OR")
    """

    documents = []

    max_results = num

    if sort_op == "relevance":
        search = arxiv.Search(
            query=search_query,
            max_results=num,
            sort_by=arxiv.SortCriterion.Relevance
        )
    elif sort_op == "lastupdate":
        search = arxiv.Search(
            query=search_query,
            max_results=num,
            sort_by=arxiv.SortCriterion.LastUpdatedDate
        )
    else:
        # ê²€ìƒ‰ ê°ì²´ ìƒì„± (ìµœì‹  ì œì¶œì¼ ìˆœìœ¼ë¡œ ì •ë ¬)
        search = arxiv.Search(
          query = search_query,
          max_results = max_results,
          sort_by = arxiv.SortCriterion.SubmittedDate
        )

    # ê²€ìƒ‰ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
    # clientì—ê²Œ resultë¼ëŠ” ì¥ë°”êµ¬ë‹ˆë¥¼ ì£¼ê³  ê±°ê¸°ì— ë°›ì•„ì˜¤ë¼ëŠ” ê²ƒ
    client = arxiv.Client()
    results = list(client.results(search))

    for result in results:
      temp_dict = {}
      # ë…¼ë¬¸ ì œëª©
      title = result.title
      temp_dict['title'] = title


      # ë…¼ë¬¸ ì €ì (ì²« ë²ˆì§¸ ì €ìë§Œ)
      first_author = result.authors[0]

      # ë…¼ë¬¸ PDF ë§í¬
      url = result.pdf_url
      temp_dict['url'] = url

      # ë…¼ë¬¸ ì´ˆë¡
      abstract = result.summary
      temp_dict['abstract'] = abstract
      # ë…¼ë¬¸ ê³ ìœ  ID (ë²„ì „ ì •ë³´ í¬í•¨)
      entry_id = result.entry_id

      # ë…¼ë¬¸ ê²Œì‹œì¼
      published_date = result.published.strftime('%Y-%m-%d')

      print(f"ğŸ“„ ì œëª©: {title}")
      print(f"ğŸ‘¤ ì €ì: {first_author}")
      print(f"ğŸ—“ï¸ ê²Œì‹œì¼: {published_date}")
      print(f"ğŸ”— ê³ ìœ  ì£¼ì†Œ: {entry_id}")
      print(f"ğŸ”— ë…¼ë¬¸ pdf ë§í¬: {url}")
      print(f"ğŸ“‹ ì´ˆë¡: {abstract[:20]}") # ì´ˆë¡ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
      print("-" * 30)

      documents.append(temp_dict)

    return documents

# ì´ í•¨ìˆ˜ëŠ” í•œ ë²ˆì— í•œ ê°œì˜ ë…¼ë¬¸ ì œëª©ì— ëŒ€í•œ citationì„ ë°›ì•„ì˜´
# crossref apië¡œ ê²€ìƒ‰ ì•ˆ ë˜ëŠ” ê²ƒë“¤ì€ ê·¸ëƒ¥ citation 0ìœ¼ë¡œ ë¦¬í„´
def get_citation(title,email):
    BASE_URL = "https://api.crossref.org/works"

    params = {
        "query.title": title,
        "rows": 1,  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ 1ê°œë§Œ ìš”ì²­
        "mailto": email  # Polite Pool ì‚¬ìš©
    }

    # 6. try...except ë¸”ë¡ì„ ì‚¬ìš©í•´ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.
    # try ë¸”ë¡ ì•ˆì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë‹¤ê°€ ì§€ì •ëœ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šê³  except ë¸”ë¡ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
    try:
        # 7. HTTP GET ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        # requests.get() í•¨ìˆ˜ê°€ URLê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°í•©í•˜ì—¬ Crossref ì„œë²„ì— ë°ì´í„°ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
        response = requests.get(BASE_URL, params=params)

        # 8. HTTP ìƒíƒœ ì½”ë“œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        # ì‘ë‹µì´ ì„±ê³µì ì´ì§€ ì•Šìœ¼ë©´ (4xx ë˜ëŠ” 5xx ì˜¤ë¥˜ ì½”ë“œ), HTTPError ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        response.raise_for_status()

        # 9. ì‘ë‹µ ë³¸ë¬¸(response body)ì„ JSONì—ì„œ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        data = response.json()

        # 10. ë°˜í™˜ëœ ë°ì´í„°ì— ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        # Crossref ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ data['message']['items'] ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ ë´…ë‹ˆë‹¤.
        if data['message']['items']:
            # 11. ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.
            item = data['message']['items'][0]

            # 12. ì¸ìš©ìˆ˜(is-referenced-by-count) ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
            # .get()ì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ í‚¤ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            citation_count = item.get('is-referenced-by-count', 0)

            # 13. ì¶”ì¶œí•œ ì¸ìš©ìˆ˜ë¥¼ í•¨ìˆ˜ì˜ ê²°ê³¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            return citation_count
        else:
            # 14. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            return 0

    # 15. ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ì…ë‹ˆë‹¤.
    # requests.get()ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•˜ê±°ë‚˜, JSON êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ë•Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    except (requests.exceptions.RequestException, KeyError, IndexError):
        # 16. ì˜¤ë¥˜ ë°œìƒ ì‹œ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        return 0


import requests


def get_citation_and_title(title_to_search, email):
    """
    ë…¼ë¬¸ ì œëª©ìœ¼ë¡œ Crossrefë¥¼ ê²€ìƒ‰í•˜ì—¬, APIê°€ ì‹¤ì œë¡œ ì°¾ì€ ë…¼ë¬¸ì˜ ì œëª©ê³¼ ì¸ìš©ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        title_to_search (str): ê²€ìƒ‰í•  ë…¼ë¬¸ ì œëª©
        email (str): Crossref APIì˜ Polite Pool ì‚¬ìš©ì„ ìœ„í•œ ì´ë©”ì¼ ì£¼ì†Œ

    Returns:
        tuple: (ì°¾ì€ ë…¼ë¬¸ ì œëª©, ì¸ìš© íšŸìˆ˜) í˜•íƒœì˜ íŠœí”Œ.
               ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ('(ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í•¨)', 0)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    BASE_URL = "https://api.crossref.org/works"
    params = {
        "query.title": title_to_search,
        "rows": 1,
        "mailto": email
    }

    try:
        response = requests.get(BASE_URL, params=params)
        response.raise_for_status()
        data = response.json()

        if data['message']['items']:
            item = data['message']['items'][0]

            # APIê°€ ì°¾ì€ ë…¼ë¬¸ì˜ ì œëª© ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ê°€ì ¸ì˜´)
            found_title = item.get('title', ['(ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ)'])[0]

            # ì¸ìš©ìˆ˜ ì¶”ì¶œ
            citation_count = item.get('is-referenced-by-count', 0)

            # ì°¾ì€ ì œëª©ê³¼ ì¸ìš©ìˆ˜ë¥¼ íŠœí”Œë¡œ ë¬¶ì–´ì„œ ë°˜í™˜
            return (found_title, citation_count)
        else:
            return ('(ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í•¨)', 0)

    except (requests.exceptions.RequestException, KeyError, IndexError):
        return ('(API ìš”ì²­ ì˜¤ë¥˜)', 0)


def sort_citation(documents, email):
    BASE_URL = "https://api.crossref.org/works"

    title_list = [doc["title"] for doc in documents]
    citation_list = []

    for title in title_list:
        citation_count = get_citation(title, email)
        citation_list.append(citation_count)

    print(citation_list)
    # 1. title_listì™€ citation_countë¥¼ zipìœ¼ë¡œ ë¬¶ì–´ì¤ë‹ˆë‹¤.
    #    ê²°ê³¼: [('Attention...', 96172), ('BERT...', 83011), ('An Image...', 47538)]
    paired_list = list(zip(documents, citation_list))

    # 2. ë¬¶ì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¸ìš©ìˆ˜(ê° ìŒì˜ ë‘ ë²ˆì§¸ ìš”ì†Œ) ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•©ë‹ˆë‹¤.
    #    key=lambda item: item[1]  --> ê° ìŒ(item)ì˜ 1ë²ˆ ì¸ë±ìŠ¤ ê°’(ì¸ìš©ìˆ˜)ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    #    reverse=True             --> ë‚´ë¦¼ì°¨ìˆœ (í° ìˆ«ìê°€ ë¨¼ì € ì˜¤ë„ë¡)
    sorted_pairs = sorted(paired_list, key=lambda item: item[1], reverse=True)

    # 3. ì •ë ¬ëœ ìŒ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œëª©(ê° ìŒì˜ ì²« ë²ˆì§¸ ìš”ì†Œ)ë§Œ ë‹¤ì‹œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    sorted_target_list = [tar for tar, criteria in sorted_pairs]


    return sorted_target_list